{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS OF CUSTIMER REVIEWS**\n",
    "This project performs sentiment analysis on customer reviews using various machine learning and deep learning models. It covers preprocessing, feature extraction, and building models like Logistic Regression, Support Vector Machines (SVM), Artificial Neural Networks (ANN), Long Short-Term Memory Networks (LSTM), and Gated Recurrent Unit Networks (GRU). The dataset consists of 150,000 labeled customer reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "In this block, all the necessary libraries for preprocessing, machine learning, deep learning, and visualization are imported. These include:\n",
    "- NLTK (Natural Language ToolKit) for natural language processing\n",
    "- Scikit-learn for machine learning models and metrics\n",
    "- TensorFlow for deep learning models\n",
    "- Matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 15:54:38.172880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append('/Users/shiven/nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the NLTK Data\n",
    "- Necessary NLTK resources like `punkt`, `wordnet`, `averaged_perceptron_tagger`, and `omw-1.4` are downloaded to ensure that tokenization, lemmatization, and POS tagging work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shiven/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/shiven/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shiven/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/shiven/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/shiven/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Lemmatizer and Helper Functions\n",
    "- **WordNet Lemmatizer**: The WordNet Lemmatizer is a tool from NLTK that reduces words to their base or root form, called the **lemma**. Unlike stemming, which simply removes prefixes or suffixes, lemmatization considers the word's meaning and its part of speech (POS) to produce a linguistically accurate root form. For example:\n",
    "  - \"running\" → \"run\" (if it's a verb)\n",
    "  - \"better\" → \"good\" (if it's an adjective)\n",
    "\n",
    "- **Why Use WordNet Lemmatizer?**\n",
    "  - It ensures that words like \"running\", \"runs\", and \"ran\" are all normalized to \"run\", improving the quality of text analysis.\n",
    "  - It works with WordNet, a lexical database for the English language, to accurately find the base form based on the word's context and POS.\n",
    "\n",
    "- **Helper Function (`get_wordnet_pos`)**:\n",
    "  - The `get_wordnet_pos` function converts POS tags (e.g., \"NN\" for nouns, \"VB\" for verbs) from the NLTK POS tagger into WordNet-compatible tags. This mapping is essential because the lemmatizer requires POS information to properly lemmatize words.\n",
    "  - Example Mapping:\n",
    "    - NLTK Tag `JJ` → WordNet Tag `ADJ` (adjective)\n",
    "    - NLTK Tag `NN` → WordNet Tag `NOUN`\n",
    "    - NLTK Tag `VB` → WordNet Tag `VERB`\n",
    "    - NLTK Tag `RB` → WordNet Tag `ADV` (adverb)\n",
    "    \n",
    "This ensures the WordNet Lemmatizer can accurately handle words based on their grammatical role in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word], lang=\"eng\")[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing with Lemmatization\n",
    "- A preprocessing function is defined to clean the text data.\n",
    "- Steps include:\n",
    "  1. Lowercasing the text.\n",
    "  2. Removing special characters and numbers.\n",
    "  3. Tokenizing the text.\n",
    "  4. Lemmatizing each token using the `get_wordnet_pos` helper function.\n",
    "- The function includes error handling to debug any issues with specific rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "        text = re.sub(r'\\d+', '', text)  \n",
    "        tokens = word_tokenize(text)  \n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]  \n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "- The dataset is loaded using `pandas`.\n",
    "- The size of the dataset and the first few rows are displayed.\n",
    "- Class distributions are printed to understand the balance between labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 150000\n",
      "   label                                               text\n",
      "0      0  Reference Yes, Learning or Instruction No: I p...\n",
      "1      0  NO!!!!!!I will give it 1 star.: This doll is n...\n",
      "2      0  Neat Features/Uncomfortable: I bought this wit...\n",
      "3      1  Progressive-Underground divas??? where's the v...\n",
      "4      0  The theif who stole a week.: What a stinker! I...\n",
      "Class counts:\n",
      "label\n",
      "0    75034\n",
      "1    74966\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('reviews.csv')   # replace with your dataset file name and path\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "print(data.head())\n",
    "\n",
    "class_counts = data['label'].value_counts()\n",
    "print(f\"Class counts:\\n{class_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Preprocessing to the Dataset\n",
    "- Null values in the `text` column are replaced with empty strings.\n",
    "- All entries in the `text` column are converted to strings.\n",
    "- The `preprocess_text` function is applied to the `text` column to clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].fillna('') \n",
    "data['text'] = data['text'].astype(str) \n",
    "data['text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- **What is Tokenization?**\n",
    "  - Tokenization is the process of breaking a stream of text into smaller units called **tokens**. These tokens can be words, phrases, or even characters, depending on the application.\n",
    "  - For example:\n",
    "    - Sentence: \"I love programming!\"\n",
    "    - Tokens: [\"I\", \"love\", \"programming\", \"!\"]\n",
    "\n",
    "  - Tokenization is a crucial preprocessing step in NLP tasks, as it transforms raw text into a structured format that can be understood by machine learning models.\n",
    "\n",
    "- **Keras's `Tokenizer`**:\n",
    "  - Keras provides a `Tokenizer` class to preprocess text data for deep learning models. It converts text into sequences of integers, where each integer represents a unique word in the vocabulary.\n",
    "  - Key Features of `Tokenizer`:\n",
    "    1. **Word Indexing**:\n",
    "       - The `Tokenizer` creates a dictionary (`word_index`) that maps each unique word to an integer.\n",
    "       - Example: {\"I\": 1, \"love\": 2, \"programming\": 3}\n",
    "    2. **Text-to-Sequence Conversion**:\n",
    "       - It converts each sentence into a sequence of integers based on the `word_index`.\n",
    "       - Example: \"I love programming\" → [1, 2, 3]\n",
    "    3. **Handles Out-of-Vocabulary Words**:\n",
    "       - Words that are not in the `word_index` can be mapped to a special \"out of vocabulary\" (OOV) token, ensuring robustness.\n",
    "    4. **Frequency-based Filtering**:\n",
    "       - You can limit the vocabulary size to the `num_words` most frequent words, which reduces noise from rare words.\n",
    "\n",
    "- **Why Use Keras's `Tokenizer`?**\n",
    "  - It simplifies the process of converting text data into numerical data, which is essential for deep learning models.\n",
    "  - It supports preprocessing for large datasets and integrates seamlessly with other Keras tools, such as embedding layers.\n",
    "\n",
    "- **Output of Tokenization**:\n",
    "  - After applying the `Tokenizer`:\n",
    "    1. Each document in the dataset is represented as a list of integers (word sequences).\n",
    "    2. The `word_index` provides a mapping of unique words to integers.\n",
    "    3. The number of unique tokens in the dataset is displayed, giving insight into the vocabulary size.\n",
    "\n",
    "This step is crucial for preparing the text data for embedding layers and other deep learning architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 218265\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Number of unique tokens: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences\n",
    "- The tokenized sequences are padded to ensure uniform length using Keras's `pad_sequences`.\n",
    "- This step is critical for feeding the data into deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GloVe Embeddings\n",
    "\n",
    "- **What is GloVe?**\n",
    "  - GloVe (Global Vectors for Word Representation) is a popular method for obtaining word embeddings, which are dense vector representations of words in a continuous vector space.\n",
    "  - Developed by researchers at Stanford, GloVe is based on the principle of using word co-occurrence statistics in a large text corpus to learn meaningful vector representations for words.\n",
    "  - Pretrained GloVe embeddings, such as `glove.6B.100d`, provide vector representations for words trained on massive corpora like Wikipedia and Gigaword datasets.\n",
    "\n",
    "- **Principle Behind GloVe**:\n",
    "  - GloVe is designed to learn word vectors such that the dot product (or cosine similarity) of two word vectors corresponds to the log-probability of their co-occurrence in a corpus.\n",
    "  - Words that frequently co-occur in similar contexts will have similar vector representations, capturing their semantic similarity.\n",
    "  - For example:\n",
    "    - Words like \"king\" and \"queen\" are closer in the embedding space because they share similar contexts in the corpus.\n",
    "    - The analogy `king - man + woman ≈ queen` arises naturally due to the embedding's structure.\n",
    "    \n",
    "- **Why Use GloVe?**\n",
    "  - **Semantic Relationships**:\n",
    "    - GloVe embeddings capture semantic similarity and relationships between words.\n",
    "    - For example, \"Paris\" - \"France\" + \"Italy\" ≈ \"Rome\".\n",
    "  - **Pretrained on Large Datasets**:\n",
    "    - Pretrained GloVe embeddings are available for various dimensions (e.g., 50d, 100d, 200d, 300d) and corpora, saving time and computational resources.\n",
    "  - **Dense and Continuous Representations**:\n",
    "    - GloVe embeddings represent each word as a fixed-length dense vector, suitable for machine learning models.\n",
    "\n",
    "- **Embedding Matrix**:\n",
    "  - After loading GloVe embeddings, an **embedding matrix** is created where:\n",
    "    - Rows correspond to words in the dataset vocabulary.\n",
    "    - Columns correspond to the vector dimensions (e.g., 100 for `glove.6B.100d`).\n",
    "  - Words missing from GloVe embeddings are initialized with zeros.\n",
    "\n",
    "- **Applications of GloVe**:\n",
    "  - Sentiment analysis, machine translation, question answering, and more.\n",
    "  - Improves model performance by providing a meaningful starting point for word representations.\n",
    "\n",
    "This step prepares the embedding matrix, which serves as input to the embedding layer in the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "glove_file = '/Users/shiven/Documents/glove.6B/glove.6B.100d.txt'\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Feature Matrix by TF-IDF\n",
    "TF = Term Frequency , IDF = Inverse Document Frequency\n",
    "tf-idf(i,j)= tf(i,j)*idf(i,j)\n",
    "\n",
    "idf(i) = log[(Total number of docs)/(Number of docs in which term occurs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(data['text'])\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- A Logistic Regression model is trained on the TF-IDF features.\n",
    "- The model's accuracy on the test set is printed.\n",
    "### Support Vector Machine (SVM)\n",
    "- A LinearSVC model is trained on the TF-IDF features.\n",
    "- The model's accuracy on the test set is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8857666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8860666666666667\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf, y_train_tfidf)\n",
    "lr_predictions = lr.predict(X_test_tfidf)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test_tfidf, lr_predictions)}\")\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_tfidf, y_train_tfidf)\n",
    "svm_predictions = svm.predict(X_test_tfidf)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test_tfidf, svm_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks ANN\n",
    "\n",
    "This code builds and trains an artificial neural network (ANN) for binary text classification using TensorFlow and Keras. The model begins with an embedding layer initialized with pre-trained embeddings (embedding_matrix), which converts input text indices into dense vectors. A global average pooling layer follows to reduce the sequence of embeddings into a single vector. Two dense layers with ReLU activation and dropout regularization are then applied to introduce non-linearity and prevent overfitting. The final output layer uses a sigmoid activation to produce a probability score for classification.\n",
    "\n",
    "The model is compiled with the Adam optimizer, binary crossentropy loss, and accuracy as the evaluation metric. Early stopping is used to monitor validation loss, halting training if no improvement is seen after three epochs while restoring the best weights. Training is conducted over a maximum of 15 epochs with a batch size of 128, leveraging training and validation data for evaluation. This setup ensures efficient training and robust performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 204ms/step - accuracy: 0.7511 - loss: 0.4787 - val_accuracy: 0.8779 - val_loss: 0.2902\n",
      "Epoch 2/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 415ms/step - accuracy: 0.8911 - loss: 0.2700 - val_accuracy: 0.8721 - val_loss: 0.3002\n",
      "Epoch 3/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 681ms/step - accuracy: 0.9109 - loss: 0.2288 - val_accuracy: 0.8839 - val_loss: 0.2789\n",
      "Epoch 4/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 900ms/step - accuracy: 0.9249 - loss: 0.1963 - val_accuracy: 0.8779 - val_loss: 0.3086\n",
      "Epoch 5/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 206ms/step - accuracy: 0.9352 - loss: 0.1728 - val_accuracy: 0.8794 - val_loss: 0.2998\n",
      "Epoch 6/15\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 202ms/step - accuracy: 0.9453 - loss: 0.1472 - val_accuracy: 0.8729 - val_loss: 0.3464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa1f684fbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model = Sequential([\n",
    "    Embedding(input_dim=len(word_index)+1, output_dim=100, weights=[embedding_matrix], trainable=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "ann_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "ann_model.fit(X_train, y_train, epochs=15, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory LSTM\n",
    "\n",
    "This code builds an LSTM-based model for binary text classification. It starts with a pre-trained, non-trainable embedding layer (embedding_matrix) that converts input indices into dense vectors. An LSTM layer with 128 units processes the sequences to capture contextual dependencies. A dense layer with 64 neurons and ReLU activation introduces non-linearity, followed by a sigmoid-activated output layer for classification.\n",
    "\n",
    "The model is compiled with the Adam optimizer, binary crossentropy loss, and accuracy as the metric. It is trained for up to 15 epochs with a batch size of 64, using training data (X_train, y_train) and validated on test data (X_test, y_test). This setup is designed for handling sequence data effectively and achieving robust classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 55ms/step - accuracy: 0.5568 - loss: 0.6800 - val_accuracy: 0.8039 - val_loss: 0.4621\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 59ms/step - accuracy: 0.8405 - loss: 0.3724 - val_accuracy: 0.8763 - val_loss: 0.2917\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 58ms/step - accuracy: 0.8813 - loss: 0.2834 - val_accuracy: 0.8827 - val_loss: 0.2804\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 59ms/step - accuracy: 0.8944 - loss: 0.2554 - val_accuracy: 0.8936 - val_loss: 0.2542\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - accuracy: 0.9068 - loss: 0.2276 - val_accuracy: 0.9010 - val_loss: 0.2398\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 59ms/step - accuracy: 0.9134 - loss: 0.2137 - val_accuracy: 0.9037 - val_loss: 0.2357\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 60ms/step - accuracy: 0.9228 - loss: 0.1957 - val_accuracy: 0.9078 - val_loss: 0.2297\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 59ms/step - accuracy: 0.9301 - loss: 0.1795 - val_accuracy: 0.9088 - val_loss: 0.2283\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 63ms/step - accuracy: 0.9392 - loss: 0.1619 - val_accuracy: 0.9081 - val_loss: 0.2304\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 57ms/step - accuracy: 0.9472 - loss: 0.1419 - val_accuracy: 0.9083 - val_loss: 0.2351\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 60ms/step - accuracy: 0.9538 - loss: 0.1284 - val_accuracy: 0.9061 - val_loss: 0.2521\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 64ms/step - accuracy: 0.9591 - loss: 0.1158 - val_accuracy: 0.9040 - val_loss: 0.2731\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 63ms/step - accuracy: 0.9649 - loss: 0.1026 - val_accuracy: 0.9037 - val_loss: 0.2767\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 64ms/step - accuracy: 0.9698 - loss: 0.0904 - val_accuracy: 0.9027 - val_loss: 0.3033\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 63ms/step - accuracy: 0.9755 - loss: 0.0764 - val_accuracy: 0.9010 - val_loss: 0.3334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa1f6c191c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=len(word_index)+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "    LSTM(128),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit GRU\n",
    "\n",
    "This code defines a Gated Recurrent Unit (GRU) model for binary text classification. It starts with a pre-trained embedding layer (embedding_matrix), which converts input indices into dense vectors. The embeddings are non-trainable, preserving the pre-trained representations. A GRU layer with 128 units follows, effectively capturing sequential dependencies in the text data.\n",
    "\n",
    "Next, a dense layer with 64 neurons and ReLU activation introduces non-linearity, followed by an output layer with a sigmoid activation function to generate probabilities for binary classification. The model is compiled using the Adam optimizer, binary crossentropy as the loss function, and accuracy as the evaluation metric.\n",
    "\n",
    "The model is trained for a maximum of 15 epochs with a batch size of 32, using the training data (X_train, y_train) and validated on the test data (X_test, y_test). This architecture is optimized for processing sequential data, ensuring effective classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 36ms/step - accuracy: 0.7102 - loss: 0.5008 - val_accuracy: 0.8908 - val_loss: 0.2685\n",
      "Epoch 2/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 36ms/step - accuracy: 0.8937 - loss: 0.2551 - val_accuracy: 0.9009 - val_loss: 0.2377\n",
      "Epoch 3/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 38ms/step - accuracy: 0.9103 - loss: 0.2204 - val_accuracy: 0.9128 - val_loss: 0.2192\n",
      "Epoch 4/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 34ms/step - accuracy: 0.9262 - loss: 0.1863 - val_accuracy: 0.9153 - val_loss: 0.2143\n",
      "Epoch 5/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 36ms/step - accuracy: 0.9380 - loss: 0.1625 - val_accuracy: 0.9143 - val_loss: 0.2186\n",
      "Epoch 6/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1937s\u001b[0m 517ms/step - accuracy: 0.9496 - loss: 0.1372 - val_accuracy: 0.9130 - val_loss: 0.2264\n",
      "Epoch 7/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 35ms/step - accuracy: 0.9587 - loss: 0.1159 - val_accuracy: 0.9102 - val_loss: 0.2502\n",
      "Epoch 8/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 35ms/step - accuracy: 0.9666 - loss: 0.0957 - val_accuracy: 0.9051 - val_loss: 0.2837\n",
      "Epoch 9/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 35ms/step - accuracy: 0.9722 - loss: 0.0818 - val_accuracy: 0.9068 - val_loss: 0.2788\n",
      "Epoch 10/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 35ms/step - accuracy: 0.9764 - loss: 0.0708 - val_accuracy: 0.9016 - val_loss: 0.3134\n",
      "Epoch 11/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 36ms/step - accuracy: 0.9791 - loss: 0.0623 - val_accuracy: 0.9037 - val_loss: 0.3107\n",
      "Epoch 12/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 37ms/step - accuracy: 0.9823 - loss: 0.0555 - val_accuracy: 0.9040 - val_loss: 0.3564\n",
      "Epoch 13/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 39ms/step - accuracy: 0.9841 - loss: 0.0501 - val_accuracy: 0.9035 - val_loss: 0.3453\n",
      "Epoch 14/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 44ms/step - accuracy: 0.9842 - loss: 0.0480 - val_accuracy: 0.9013 - val_loss: 0.3644\n",
      "Epoch 15/15\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 43ms/step - accuracy: 0.9862 - loss: 0.0424 - val_accuracy: 0.9022 - val_loss: 0.3847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa1dc8bb130>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=len(word_index)+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "    GRU(128),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison\n",
    "- The accuracies of Logistic Regression, SVM, ANN, LSTM, and GRU are compared.\n",
    "- A bar plot is created to visualize the performance of each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_accuracy = accuracy_score(y_test_tfidf, lr_predictions)\n",
    "svm_accuracy = accuracy_score(y_test_tfidf, svm_predictions)\n",
    "ann_accuracy = ann_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "gru_accuracy = gru_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "model_names = ['Logistic Regression', 'SVM', 'ANN', 'LSTM', 'GRU']\n",
    "accuracies = [lr_accuracy, svm_accuracy, ann_accuracy, lstm_accuracy, gru_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, accuracies, alpha=0.7, edgecolor='black')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function\n",
    "- A function is defined to predict sentiment for user-input text.\n",
    "- The text is preprocessed, tokenized, and padded before being fed into the LSTM model.\n",
    "- The predicted sentiment (positive or negative) and confidence score are displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Review: Absolutely fantastic! The product exceeded my expectations in every way. Highly recommended!\n",
      "Sentiment: Positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Review: Terrible experience. The product stopped working after just one use. Very disappointed.\n",
      "Sentiment: Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Review: The quality is awful, and it doesn’t match the description at all. Waste of money.\n",
      "Sentiment: Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Review: I was skeptical at first, but this product has completely won me over. From the packaging to the performance, everything about it screams quality. It arrived on time, and setting it up was a breeze thanks to the detailed instructions provided. I've been using it for a few weeks now, and it’s been working flawlessly without any issues. The materials feel premium, and it looks great too. What impressed me the most is how well it performs under heavy use—truly above and beyond my expectations. Plus, the customer service team was incredibly responsive when I had a minor question. If you're on the fence about buying this, just go for it—you won't regret it!\n",
      "Sentiment: Positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Review: Do not buy this! It’s overpriced and breaks easily. Horrible quality.\n",
      "Sentiment: Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Review: Amazing value for the price! This is by far one of the best purchases I've ever made.\n",
      "Sentiment: Positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Review: I’ve never been more disappointed with a purchase. The product looked good in the photos, but in reality, it’s poorly made and feels cheap. It worked fine for the first few days, but then it started malfunctioning. Now it’s completely unusable, and I’m left frustrated. To make matters worse, the warranty policy is a joke—they refused to replace it because they claimed the damage was my fault, even though I followed all the instructions. I’ve wasted my hard-earned money on this, and I wouldn’t recommend it to anyone. It’s such a shame because I had really hoped this would meet my needs.\n",
      "Sentiment: Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Review: I regret buying this. It’s poorly made, and the customer support is unhelpful.\n",
      "Sentiment: Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Review: I love this! It’s exactly what I was looking for and performs flawlessly.\n",
      "Sentiment: Positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Review: I had high hopes for this product based on the reviews, but it turned out to be a complete disaster. It arrived late, and when I finally opened the package, I noticed scratches and dents on the surface—it looked like it had already been used. Setting it up was a nightmare because the instructions were unclear and missing key details. After spending hours trying to get it to work, I discovered that some parts were defective. The worst part? When I reached out to customer support, they took forever to respond and were unhelpful when they finally did. For the price I paid, I expected much better quality and service. Save yourself the trouble and look for a better option.\n",
      "Sentiment: Negative\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(user_input, model, tokenizer, max_length):\n",
    "    processed_input = preprocess_text(user_input)\n",
    "    sequence = tokenizer.texts_to_sequences([processed_input])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
    "    return sentiment, prediction[0][0]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter a review (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    sentiment, confidence = predict_sentiment(user_input, lstm_model, tokenizer, max_length)\n",
    "    print(f\"Review: {user_input}\")\n",
    "    print(f\"Sentiment: {sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
